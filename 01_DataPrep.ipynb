{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to prep the data for the alignment task.  This includes computing audio features and generating a query list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa as lb\n",
    "import pickle as pkl\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import scipy.io.wavfile as wav\n",
    "from math import sqrt\n",
    "import IPython.display as ipd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Change this cell to suit your file structure #####\n",
    "MAZURKAS_ROOT = Path('/data/Datasets/Chopin_Mazurkas') # Path to Mazurkas dataset root directory\n",
    "OUT_ROOT = Path().absolute() # Output root directory (this is where features, paths, etc. will be saved)\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_ROOT = MAZURKAS_ROOT / 'annotations_beat'\n",
    "AUDIO_ROOT = MAZURKAS_ROOT / 'wav_22050_mono'\n",
    "FEATURES_ROOT = OUT_ROOT / 'features'\n",
    "train_files = Path('cfg_files/filelist.train.txt')\n",
    "test_files = Path('cfg_files/filelist.test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(FEATURES_ROOT):\n",
    "    os.mkdir(FEATURES_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute features on clean audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute features on the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chroma_single(infile, outfile, sr = 22050, hop_length=512):\n",
    "    '''Compute and save the chroma features for a single audio file'''\n",
    "    y, sr = lb.core.load(infile, sr = sr)\n",
    "    #F = lb.feature.chroma_cens(y, sr=sr, hop_length=hop_length)\n",
    "    F = lb.feature.chroma_cqt(y, sr=sr, hop_length=hop_length, norm=2)\n",
    "    np.save(outfile, F)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chroma_batch(filelist, outdir, n_cores):\n",
    "    '''Compute and save the chroma features for all files in a filelist'''\n",
    "    # prep inputs for parallelization\n",
    "    inputs = []\n",
    "    with open(filelist, 'r') as f:\n",
    "        for line in f:\n",
    "            relpath = line.strip()\n",
    "            reldir, fileid = os.path.split(relpath)\n",
    "            featdir = outdir / reldir\n",
    "            featdir.mkdir(parents=True, exist_ok=True)\n",
    "            featfile = (featdir / fileid).with_suffix('.npy')\n",
    "            audiofile = (AUDIO_ROOT / relpath).with_suffix('.wav')\n",
    "            if os.path.exists(featfile):\n",
    "                print(f\"Skipping {featfile}\")\n",
    "            else:\n",
    "                inputs.append((audiofile, featfile))\n",
    "\n",
    "    # process files in parallel\n",
    "    pool = multiprocessing.Pool(processes = n_cores)\n",
    "    pool.starmap(compute_chroma_single, inputs)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATS_CLEAN_DIR = FEATURES_ROOT / 'clean'\n",
    "compute_chroma_batch(train_files, FEATS_CLEAN_DIR, 24)\n",
    "compute_chroma_batch(test_files, FEATS_CLEAN_DIR, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate query list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate a file containing each pair of files to be aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_list(filelist, outfile):\n",
    "    \n",
    "    # group files by piece\n",
    "    d = {}\n",
    "    with open(filelist, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('/')\n",
    "            assert len(parts) == 2\n",
    "            piece, fileid = parts\n",
    "            if piece not in d:\n",
    "                d[piece] = []\n",
    "            d[piece].append(fileid)\n",
    "            \n",
    "    # print out all pairings\n",
    "    with open(outfile, 'w') as fout:\n",
    "        for piece in d:\n",
    "            num_recordings = len(d[piece])\n",
    "            for i in range(num_recordings):\n",
    "                fileid1 = d[piece][i]\n",
    "                for j in range(i+1, num_recordings):\n",
    "                    fileid2 = d[piece][j]\n",
    "                    line = f'{piece}/{fileid1} {piece}/{fileid2}\\n'\n",
    "                    fout.write(line)\n",
    "                    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries = 'cfg_files/query.train.list'\n",
    "test_queries = 'cfg_files/query.test.list'\n",
    "generate_query_list(train_files, train_queries)\n",
    "generate_query_list(test_files, test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNoisyData(clean, outdir, SNR):\n",
    "    '''\n",
    "    clean -- Directory of clean data\n",
    "    outdir -- Output directory of noisy data\n",
    "    SNR -- Desired SNR in dB\n",
    "    '''\n",
    "    # Set up file structure\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "\n",
    "    if not os.path.exists(outdir / \"wav_22050_mono\"):\n",
    "        os.mkdir(outdir / \"wav_22050_mono\")\n",
    "    \n",
    "    cleanDirList = glob.glob(str(clean) + \"/wav_22050_mono/*\")\n",
    "    for cleanDir in cleanDirList:\n",
    "        newDir = outdir / \"wav_22050_mono\" / os.path.basename(cleanDir)\n",
    "        if not os.path.exists(newDir):\n",
    "            os.mkdir(newDir)\n",
    "    \n",
    "    # Fill up annotation directory\n",
    "    os.system(\"cp -r \"+ str(clean) + \"/annotations_beat \"+ str(outdir) + \"/annotations_beat\")\n",
    "    \n",
    "    # Add noise to all files\n",
    "    for cleanDir in cleanDirList:\n",
    "        cleanFileList = glob.glob(cleanDir + \"/*\")\n",
    "        for cleanFile in cleanFileList:\n",
    "            rate, cleanAudio = wav.read(cleanFile)\n",
    "            # Need to make sure the dtype isn't too small to handle the squares when finding the power\n",
    "            cleanAudio = np.array(cleanAudio, dtype = np.int64)\n",
    "            \n",
    "            noisyAudio = addNoise(cleanAudio, SNR)\n",
    "            \n",
    "            relPath = os.path.relpath(cleanFile, start=clean)\n",
    "            noisyFile = outdir / relPath\n",
    "            wav.write(noisyFile, rate, noisyAudio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNoise(cleanAudio, SNR):\n",
    "    \"\"\"Add SNR to clean audio\"\"\"\n",
    "    P_signal = np.sum(cleanAudio*cleanAudio)/len(cleanAudio)\n",
    "    P_noise = P_signal * (10 ** (-1 * SNR / 10))\n",
    "    # P_noise = sigma ^ 2\n",
    "    noise = np.random.normal(size = len(cleanAudio)) * sqrt(P_noise)\n",
    "    noisyAudio = cleanAudio + noise\n",
    "    return noisyAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noiseLevels = [20, 15, 10, 5, 0, -5, -10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"noisyData\"):\n",
    "    os.mkdir(\"noisyData\")\n",
    "\n",
    "# Add noise to Chopin Mazurka data\n",
    "for SNR in noiseLevels:\n",
    "    clean = MAZURKAS_ROOT\n",
    "    outdir = OUT_ROOT / (\"noisyData/Chopin_Mazurkas_Noisy_%sdB\" % str(SNR))\n",
    "    generateNoisyData(clean, outdir, SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(processes = 10)\n",
    "# Compute chroma features for noisy data\n",
    "for SNR in [20, 15, 10, 5, 0, -5, -10]:\n",
    "    FEATS_DIR = FEATURES_ROOT / ('noisy_%sdB' % str(SNR))\n",
    "    if not os.path.exists(FEATS_DIR):\n",
    "        os.mkdir(FEATS_DIR)\n",
    "    \n",
    "    dirList = glob.glob(str(OUT_ROOT) + \"noisyData/Chopin_Mazurkas_Noisy_%sdB/wav_22050_mono/*\" % str(SNR))\n",
    "    for inDir in dirList:\n",
    "        outdir = FEATS_DIR / os.path.basename(inDir)\n",
    "        if not os.path.exists(outdir):\n",
    "            os.mkdir(outdir)\n",
    "        filelist = glob.glob(inDir + \"/*\")\n",
    "        # prep inputs for parallelization\n",
    "        inputs = []\n",
    "        for file in filelist:\n",
    "            outfile = outdir / (os.path.basename(file)[:-4])\n",
    "            if os.path.exists(outfile.with_suffix(\".npy\")):\n",
    "                continue\n",
    "            else:\n",
    "                inputs.append((file, outfile))\n",
    "\n",
    "        # process files in parallel\n",
    "        pool.starmap(compute_chroma_single, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Feature Matrices for Runtime Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveRandomFeatureMatrices(sizes, outdir):\n",
    "    \n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    for sz in sizes:\n",
    "        F = np.random.rand(12, sz)\n",
    "        outfile = outdir / ('F_%s.npy' % sz)\n",
    "        np.save(outfile, F)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [1000, 2000, 5000, 10000, 20000, 50000]\n",
    "rand_feat_dir = FEATURES_ROOT / 'random'\n",
    "saveRandomFeatureMatrices(sizes, rand_feat_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newSegDTW",
   "language": "python",
   "name": "newsegdtw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
